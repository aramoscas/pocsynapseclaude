#!/bin/bash

echo "üîß Correction du probl√®me aioredis TimeoutError..."

# Le probl√®me : aioredis 2.0.x a un conflit avec Python 3.11
# Solution : Utiliser redis-py avec support asyncio au lieu d'aioredis

echo "üìù 1. Mise √† jour des workers pour utiliser redis.asyncio..."

# Nouveau Dispatcher avec redis.asyncio
cat > services/dispatcher/main.py << 'EOF'
#!/usr/bin/env python3
import asyncio
import json
import logging
from datetime import datetime
import redis.asyncio as redis

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DispatcherWorker:
    def __init__(self):
        self.redis = None
        self.running = False
    
    async def start(self):
        try:
            # Connexion Redis avec redis.asyncio (plus stable que aioredis)
            self.redis = redis.Redis(host='redis', port=6379, decode_responses=True)
            await self.redis.ping()
            self.running = True
            logger.info("‚úÖ Dispatcher Worker started and connected to Redis")
            
            # D√©marrer la boucle de traitement
            await self.process_jobs()
            
        except Exception as e:
            logger.error(f"‚ùå Dispatcher startup failed: {e}")
            raise
    
    async def process_jobs(self):
        logger.info("üîÑ Starting job processing loop...")
        
        while self.running:
            try:
                # R√©cup√©rer un job de la queue (bloquant avec timeout)
                job_data = await self.redis.brpop("jobs:queue:eu-west-1", timeout=5)
                
                if job_data:
                    queue_name, job_json = job_data
                    job = json.loads(job_json)
                    job_id = job.get('job_id', 'unknown')
                    model_name = job.get('model_name', 'unknown')
                    
                    logger.info(f"üöÄ Processing job {job_id} - Model: {model_name}")
                    
                    # Traiter le job
                    await self.execute_job(job)
                    
                else:
                    # Timeout - pas de jobs disponibles
                    logger.debug("No jobs in queue, waiting...")
                    
            except Exception as e:
                logger.error(f"‚ùå Job processing error: {e}")
                await asyncio.sleep(5)
    
    async def execute_job(self, job):
        """Ex√©cuter un job et envoyer le r√©sultat"""
        job_id = job.get('job_id')
        
        try:
            # Simuler ex√©cution AI (temps variable selon le mod√®le)
            start_time = asyncio.get_event_loop().time()
            
            model_name = job.get('model_name', 'unknown')
            if model_name == 'resnet50':
                await asyncio.sleep(1.5)  # Simulation ResNet50
                predictions = [0.85, 0.15]
            elif model_name == 'gpt-3.5':
                await asyncio.sleep(2.5)  # Simulation GPT
                predictions = [0.92, 0.08]
            else:
                await asyncio.sleep(1.0)  # Default
                predictions = [0.75, 0.25]
            
            end_time = asyncio.get_event_loop().time()
            execution_time = (end_time - start_time) * 1000  # en ms
            
            # Cr√©er r√©sultat
            result = {
                "job_id": job_id,
                "status": "completed",
                "result": {
                    "model": model_name,
                    "predictions": predictions,
                    "confidence": predictions[0],
                    "processing_time_ms": round(execution_time)
                },
                "node_id": "dispatcher_worker",
                "completed_at": datetime.utcnow().isoformat(),
                "execution_time_ms": round(execution_time)
            }
            
            # Envoyer r√©sultat vers l'aggregator
            await self.redis.lpush("job_results", json.dumps(result))
            
            logger.info(f"‚úÖ Job {job_id} completed in {execution_time:.0f}ms - Result sent to aggregator")
            
        except Exception as e:
            # Job failed
            error_result = {
                "job_id": job_id,
                "status": "failed",
                "error": str(e),
                "node_id": "dispatcher_worker",
                "completed_at": datetime.utcnow().isoformat()
            }
            
            await self.redis.lpush("job_results", json.dumps(error_result))
            logger.error(f"‚ùå Job {job_id} failed: {e}")

async def main():
    dispatcher = DispatcherWorker()
    try:
        await dispatcher.start()
    except KeyboardInterrupt:
        logger.info("üõë Dispatcher Worker shutdown")
        dispatcher.running = False
    except Exception as e:
        logger.error(f"‚ùå Dispatcher Worker error: {e}")

if __name__ == "__main__":
    asyncio.run(main())
EOF

# Nouveau Aggregator avec redis.asyncio
cat > services/aggregator/main.py << 'EOF'
#!/usr/bin/env python3
import asyncio
import json
import logging
from datetime import datetime
import redis.asyncio as redis

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AggregatorWorker:
    def __init__(self):
        self.redis = None
        self.running = False
    
    async def start(self):
        try:
            # Connexion Redis avec redis.asyncio
            self.redis = redis.Redis(host='redis', port=6379, decode_responses=True)
            await self.redis.ping()
            self.running = True
            logger.info("‚úÖ Aggregator Worker started and connected to Redis")
            
            # D√©marrer la boucle d'agr√©gation
            await self.process_results()
            
        except Exception as e:
            logger.error(f"‚ùå Aggregator startup failed: {e}")
            raise
    
    async def process_results(self):
        logger.info("üìä Starting result processing loop...")
        
        while self.running:
            try:
                # R√©cup√©rer un r√©sultat de la queue (bloquant avec timeout)
                result_data = await self.redis.brpop("job_results", timeout=5)
                
                if result_data:
                    queue_name, result_json = result_data
                    result = json.loads(result_json)
                    job_id = result.get('job_id', 'unknown')
                    status = result.get('status', 'unknown')
                    
                    logger.info(f"üìä Processing result for job {job_id} - Status: {status}")
                    
                    # Stocker le r√©sultat
                    await self.store_result(result)
                    
                else:
                    # Timeout - pas de r√©sultats disponibles
                    logger.debug("No results to process, waiting...")
                    
            except Exception as e:
                logger.error(f"‚ùå Result processing error: {e}")
                await asyncio.sleep(5)
    
    async def store_result(self, result):
        """Stocker le r√©sultat pour r√©cup√©ration par le client"""
        job_id = result.get('job_id')
        
        try:
            # Stocker r√©sultat avec TTL de 1 heure (3600 secondes)
            result_key = f"result:{job_id}"
            await self.redis.setex(result_key, 3600, json.dumps(result))
            
            # Stocker aussi dans une liste des r√©sultats r√©cents
            await self.redis.lpush("recent_results", json.dumps({
                "job_id": job_id,
                "status": result.get('status'),
                "completed_at": result.get('completed_at'),
                "execution_time_ms": result.get('execution_time_ms')
            }))
            
            # Garder seulement les 100 derniers r√©sultats
            await self.redis.ltrim("recent_results", 0, 99)
            
            # Mettre √† jour les m√©triques
            await self.update_metrics(result)
            
            logger.info(f"‚úÖ Result stored for job {job_id} - Available at result:{job_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to store result for {job_id}: {e}")
    
    async def update_metrics(self, result):
        """Mettre √† jour les m√©triques syst√®me"""
        try:
            # Incr√©menter compteur jobs compl√©t√©s
            await self.redis.incr("metrics:completed_jobs")
            
            # Mettre √† jour latence moyenne (simple)
            execution_time = result.get('execution_time_ms', 0)
            if execution_time > 0:
                await self.redis.lpush("metrics:latencies", execution_time)
                await self.redis.ltrim("metrics:latencies", 0, 99)  # Garder 100 mesures
            
            # Mettre √† jour timestamp derni√®re activit√©
            await self.redis.set("metrics:last_activity", datetime.utcnow().isoformat())
            
            logger.debug(f"Metrics updated - execution_time: {execution_time}ms")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to update metrics: {e}")

async def main():
    aggregator = AggregatorWorker()
    try:
        await aggregator.start()
    except KeyboardInterrupt:
        logger.info("üõë Aggregator Worker shutdown")
        aggregator.running = False
    except Exception as e:
        logger.error(f"‚ùå Aggregator Worker error: {e}")

if __name__ == "__main__":
    asyncio.run(main())
EOF

echo "üìù 2. Mise √† jour des Dockerfiles pour utiliser redis au lieu d'aioredis..."

# Nouveau Dockerfile dispatcher
cat > services/dispatcher/Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Installer redis (stable avec Python 3.11)
RUN pip install --no-cache-dir redis[hiredis]

# Copier le worker
COPY main.py .

# Lancer le worker
CMD ["python", "main.py"]
EOF

# Nouveau Dockerfile aggregator
cat > services/aggregator/Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Installer redis (stable avec Python 3.11)  
RUN pip install --no-cache-dir redis[hiredis]

# Copier le worker
COPY main.py .

# Lancer le worker
CMD ["python", "main.py"]
EOF

echo "üìù 3. Rebuild des images avec la correction..."

# Build dispatcher
cd services/dispatcher
docker build --no-cache -t synapsegrid-dispatcher .
cd ../aggregator
docker build --no-cache -t synapsegrid-aggregator .
cd ../..

echo "üìù 4. Red√©marrage des services corrig√©s..."

# Arr√™ter les anciens conteneurs
docker-compose stop dispatcher aggregator
docker-compose rm -f dispatcher aggregator

# D√©marrer avec les nouvelles images
docker-compose up -d dispatcher aggregator

echo "‚è≥ Attente d√©marrage des workers corrig√©s..."
sleep 10

echo "üìù 5. Test des workers corrig√©s..."

echo "üìã Logs Dispatcher (derni√®res lignes):"
docker-compose logs --tail=8 dispatcher

echo ""
echo "üìã Logs Aggregator (derni√®res lignes):"
docker-compose logs --tail=8 aggregator

echo ""
echo "üß™ Test de soumission job..."
response=$(curl -s -X POST http://localhost:8080/submit \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer test-token" \
    -H "X-Client-ID: test-client" \
    -d '{"model_name": "resnet50", "input_data": {"test": "data"}, "priority": 5}')

if echo "$response" | grep -q "job_id"; then
    job_id=$(echo "$response" | grep -o '"job_id":"[^"]*"' | cut -d'"' -f4)
    echo "‚úÖ Job soumis: $job_id"
    
    echo "‚è≥ Attente traitement (8 secondes)..."
    sleep 8
    
    echo "üìä √âtat des queues Redis:"
    echo "  Jobs queue: $(docker-compose exec -T redis redis-cli llen jobs:queue:eu-west-1)"
    echo "  Results queue: $(docker-compose exec -T redis redis-cli llen job_results)"
    
    echo ""
    echo "üîç V√©rification r√©sultat final:"
    result=$(docker-compose exec -T redis redis-cli get "result:$job_id" 2>/dev/null)
    if [ -n "$result" ] && [ "$result" != "(nil)" ]; then
        echo "üéâ R√âSULTAT TROUV√â !"
        echo "$result" | python3 -m json.tool 2>/dev/null || echo "$result"
    else
        echo "‚è≥ R√©sultat pas encore disponible"
        
        echo "Debug des queues:"
        echo "Jobs restants: $(docker-compose exec -T redis redis-cli llen jobs:queue:eu-west-1)"
        echo "Results en attente: $(docker-compose exec -T redis redis-cli llen job_results)"
    fi
    
else
    echo "‚ùå √âchec soumission job"
    echo "R√©ponse: $response"
fi

echo ""
echo "‚úÖ Correction aioredis termin√©e !"
echo ""
echo "Les workers devraient maintenant fonctionner correctement avec redis.asyncio"
