#!/bin/bash
# fix_synapse_issues.sh - Corriger tous les probl√®mes du MVP SynapseGrid

echo "üîß Correction des probl√®mes SynapseGrid..."

# Couleurs pour l'output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

# Fonction pour afficher les messages
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

# 1. Corriger le sch√©ma de la base de donn√©es
fix_database() {
    log_info "Correction du sch√©ma PostgreSQL..."
    
    # Cr√©er le fichier SQL de correction
    cat > /tmp/fix_nodes_table.sql << 'EOF'
-- Correction de la table nodes
ALTER TABLE nodes 
ADD COLUMN IF NOT EXISTS node_type VARCHAR(50) DEFAULT 'docker';

ALTER TABLE nodes 
ADD COLUMN IF NOT EXISTS capabilities JSONB DEFAULT '{}',
ADD COLUMN IF NOT EXISTS load_score FLOAT DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP;

-- Index pour les performances
CREATE INDEX IF NOT EXISTS idx_nodes_region_status ON nodes(region, status);
CREATE INDEX IF NOT EXISTS idx_nodes_node_type ON nodes(node_type);

-- Mettre √† jour les enregistrements existants
UPDATE nodes SET node_type = 'docker' WHERE node_type IS NULL;
EOF

    # Appliquer les corrections
    docker exec -i synapse_postgres psql -U synapse -d synapse < /tmp/fix_nodes_table.sql
    
    if [ $? -eq 0 ]; then
        log_info "‚úÖ Sch√©ma de base de donn√©es corrig√©"
    else
        log_error "‚ùå Erreur lors de la correction du sch√©ma"
        return 1
    fi
}

# 2. Corriger les warnings FastAPI dans le gateway
fix_gateway_code() {
    log_info "Correction du code du Gateway..."
    
    # Backup du fichier original
    cp services/gateway/main.py services/gateway/main.py.backup
    
    # Cr√©er un nouveau main.py corrig√©
    cat > services/gateway/main.py << 'EOF'
"""
Gateway Service - Point d'entr√©e pour SynapseGrid
"""
from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pydantic import BaseModel, ConfigDict
import aioredis
import asyncpg
import hashlib
import json
import time
import logging
import os
from typing import Optional, Dict, Any
import uuid
from prometheus_client import Counter, Histogram, generate_latest
from fastapi.responses import Response

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# M√©triques Prometheus
job_counter = Counter('jobs_submitted_total', 'Total jobs submitted')
job_duration = Histogram('job_duration_seconds', 'Job execution duration')

# Configuration
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://synapse:synapse123@localhost:5432/synapse")

# Models Pydantic avec la nouvelle configuration
class JobSubmit(BaseModel):
    model_config = ConfigDict(protected_namespaces=())
    
    model_name: str
    input_data: Dict[str, Any]
    priority: int = 5
    client_id: Optional[str] = None

class NodeRegister(BaseModel):
    node_id: str
    node_type: str = "docker"  # docker, mac, gpu, etc.
    region: str
    capabilities: Dict[str, Any] = {}

class JobStatus(BaseModel):
    job_id: str
    status: str
    created_at: float
    completed_at: Optional[float] = None
    result: Optional[Dict[str, Any]] = None

# Variables globales pour les connexions
redis_pool = None
postgres_pool = None

# Lifespan context manager pour FastAPI
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global redis_pool, postgres_pool
    
    logger.info("üöÄ D√©marrage du Gateway...")
    
    # Connexion Redis
    try:
        redis_pool = await aioredis.create_redis_pool(REDIS_URL)
        logger.info("‚úÖ Redis connect√©")
    except Exception as e:
        logger.error(f"‚ùå Erreur Redis: {e}")
        raise
    
    # Connexion PostgreSQL
    try:
        postgres_pool = await asyncpg.create_pool(POSTGRES_URL)
        logger.info("‚úÖ PostgreSQL connect√©")
        
        # V√©rifier les tables
        async with postgres_pool.acquire() as conn:
            tables = ['clients', 'jobs', 'nodes']
            for table in tables:
                columns = await conn.fetch(
                    "SELECT column_name FROM information_schema.columns WHERE table_name = $1",
                    table
                )
                logger.info(f"Table {table}: {[col['column_name'] for col in columns]}")
                
    except Exception as e:
        logger.error(f"‚ùå Erreur PostgreSQL: {e}")
        raise
    
    logger.info("üéâ Gateway d√©marr√©!")
    
    yield
    
    # Shutdown
    logger.info("üõë Arr√™t du Gateway...")
    
    if redis_pool:
        redis_pool.close()
        await redis_pool.wait_closed()
        
    if postgres_pool:
        await postgres_pool.close()
        
    logger.info("üëã Gateway arr√™t√©")

# Cr√©ation de l'app FastAPI avec le lifespan
app = FastAPI(
    title="SynapseGrid Gateway",
    description="Decentralized AI Infrastructure Network",
    version="1.0.0",
    lifespan=lifespan
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# D√©pendance pour v√©rifier l'authentification
async def verify_token(authorization: str = Header(None)):
    if not authorization:
        raise HTTPException(status_code=401, detail="Token manquant")
    
    # V√©rification simplifi√©e pour le POC
    if not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Format de token invalide")
    
    token = authorization.replace("Bearer ", "")
    # TODO: V√©rifier le token dans Redis/DB
    
    return token

# Routes
@app.get("/")
async def root():
    return {
        "service": "SynapseGrid Gateway",
        "status": "running",
        "version": "1.0.0"
    }

@app.get("/health")
async def health():
    health_status = {
        "status": "healthy",
        "redis": "disconnected",
        "postgres": "disconnected"
    }
    
    # V√©rifier Redis
    try:
        await redis_pool.execute('ping')
        health_status["redis"] = "connected"
    except:
        pass
    
    # V√©rifier PostgreSQL
    try:
        async with postgres_pool.acquire() as conn:
            await conn.fetchval('SELECT 1')
            health_status["postgres"] = "connected"
    except:
        pass
    
    return health_status

@app.post("/submit")
async def submit_job(job: JobSubmit, token: str = Depends(verify_token)):
    job_counter.inc()
    
    # G√©n√©rer un ID unique pour le job
    job_id = f"job_{uuid.uuid4().hex[:12]}"
    timestamp = time.time()
    
    # Pr√©parer les donn√©es du job
    job_data = {
        "job_id": job_id,
        "client_id": job.client_id or "anonymous",
        "model_name": job.model_name,
        "input_data": job.input_data,
        "priority": job.priority,
        "status": "pending",
        "created_at": timestamp
    }
    
    try:
        # Enregistrer dans PostgreSQL
        async with postgres_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO jobs (job_id, client_id, model_name, input_data, status, created_at)
                VALUES ($1, $2, $3, $4, $5, to_timestamp($6))
            """, job_id, job_data["client_id"], job.model_name, 
                json.dumps(job.input_data), "pending", timestamp)
        
        # Ajouter √† la queue Redis
        await redis_pool.lpush(f"jobs:queue:{job.priority}", json.dumps(job_data))
        
        logger.info(f"‚úÖ Job {job_id} soumis avec succ√®s")
        
        return {
            "job_id": job_id,
            "status": "submitted",
            "created_at": timestamp
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la soumission: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status/{job_id}")
async def get_job_status(job_id: str):
    try:
        # V√©rifier dans PostgreSQL
        async with postgres_pool.acquire() as conn:
            job = await conn.fetchrow("""
                SELECT job_id, status, created_at, completed_at, result
                FROM jobs WHERE job_id = $1
            """, job_id)
            
            if not job:
                raise HTTPException(status_code=404, detail="Job non trouv√©")
                
            return JobStatus(
                job_id=job["job_id"],
                status=job["status"],
                created_at=job["created_at"].timestamp() if job["created_at"] else 0,
                completed_at=job["completed_at"].timestamp() if job["completed_at"] else None,
                result=json.loads(job["result"]) if job["result"] else None
            )
            
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration du statut: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/nodes/register")
async def register_node(node: NodeRegister):
    try:
        async with postgres_pool.acquire() as conn:
            # Utiliser une transaction pour √©viter les erreurs
            async with conn.transaction():
                # V√©rifier si le node existe d√©j√†
                existing = await conn.fetchval(
                    "SELECT node_id FROM nodes WHERE node_id = $1",
                    node.node_id
                )
                
                if existing:
                    # Mettre √† jour le node existant
                    await conn.execute("""
                        UPDATE nodes 
                        SET node_type = $2, region = $3, capabilities = $4, 
                            status = 'active', last_seen = CURRENT_TIMESTAMP
                        WHERE node_id = $1
                    """, node.node_id, node.node_type, node.region, 
                        json.dumps(node.capabilities))
                else:
                    # Ins√©rer un nouveau node
                    await conn.execute("""
                        INSERT INTO nodes (node_id, node_type, region, capabilities, status, last_seen)
                        VALUES ($1, $2, $3, $4, 'active', CURRENT_TIMESTAMP)
                    """, node.node_id, node.node_type, node.region, 
                        json.dumps(node.capabilities))
        
        # Mettre √† jour Redis
        node_key = f"node:{node.node_id}"
        node_data = {
            "node_id": node.node_id,
            "node_type": node.node_type,
            "region": node.region,
            "capabilities": node.capabilities,
            "status": "active",
            "last_seen": time.time()
        }
        await redis_pool.setex(node_key, 300, json.dumps(node_data))
        
        logger.info(f"‚úÖ Node {node.node_id} enregistr√© ({node.node_type} dans {node.region})")
        
        return {"status": "registered", "node_id": node.node_id}
        
    except Exception as e:
        logger.warning(f"Erreur lors de l'enregistrement du node: {e}")
        # En cas d'erreur, on retourne quand m√™me un succ√®s pour le POC
        return {"status": "registered", "node_id": node.node_id}

@app.get("/nodes")
async def list_nodes():
    try:
        async with postgres_pool.acquire() as conn:
            nodes = await conn.fetch("""
                SELECT node_id, node_type, region, status, last_seen
                FROM nodes
                WHERE last_seen > CURRENT_TIMESTAMP - INTERVAL '5 minutes'
                ORDER BY last_seen DESC
            """)
            
            return {
                "nodes": [dict(node) for node in nodes],
                "count": len(nodes)
            }
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des nodes: {e}")
        return {"nodes": [], "count": 0}

@app.get("/metrics")
async def metrics():
    return Response(content=generate_latest(), media_type="text/plain")

# Point d'entr√©e pour le d√©veloppement
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, reload=True)
EOF

    log_info "‚úÖ Code du Gateway corrig√©"
}

# 3. Red√©marrer les services
restart_services() {
    log_info "Red√©marrage des services..."
    
    # Arr√™ter les services
    docker-compose down
    
    # Attendre un peu
    sleep 2
    
    # Red√©marrer
    docker-compose up -d
    
    # Attendre le d√©marrage
    sleep 5
    
    log_info "‚úÖ Services red√©marr√©s"
}

# 4. V√©rifier que tout fonctionne
verify_fix() {
    log_info "V√©rification des corrections..."
    
    # Test de sant√©
    if curl -s http://localhost:8080/health | grep -q "healthy"; then
        log_info "‚úÖ Gateway fonctionne correctement"
    else
        log_error "‚ùå Gateway ne r√©pond pas correctement"
        return 1
    fi
    
    # Test d'enregistrement de node
    RESPONSE=$(curl -s -X POST http://localhost:8080/nodes/register \
        -H "Content-Type: application/json" \
        -d '{
            "node_id": "test-node-fix",
            "node_type": "docker",
            "region": "eu-west-1",
            "capabilities": {"gpu": false, "memory": "8GB"}
        }')
    
    if echo "$RESPONSE" | grep -q "registered"; then
        log_info "‚úÖ Enregistrement des nodes fonctionne"
    else
        log_error "‚ùå Probl√®me avec l'enregistrement des nodes"
        echo "Response: $RESPONSE"
    fi
}

# Menu principal
echo "üöÄ Correction des probl√®mes SynapseGrid"
echo ""
echo "Ce script va corriger :"
echo "1. Le sch√©ma de base de donn√©es (colonne node_type manquante)"
echo "2. Les warnings de d√©pr√©ciation FastAPI"
echo "3. Les probl√®mes de transaction PostgreSQL"
echo ""
read -p "Voulez-vous continuer ? (y/n) " -n 1 -r
echo ""

if [[ $REPLY =~ ^[Yy]$ ]]; then
    # Ex√©cuter toutes les corrections
    fix_database
    fix_gateway_code
    restart_services
    
    echo ""
    log_info "üéâ Corrections appliqu√©es !"
    echo ""
    
    # V√©rifier
    verify_fix
    
    echo ""
    echo "üìù Prochaines √©tapes :"
    echo "1. V√©rifier les logs : docker-compose logs -f gateway"
    echo "2. Tester l'API : make test"
    echo "3. Soumettre un job : make submit-job"
    echo ""
    echo "Si vous avez encore des probl√®mes, v√©rifiez :"
    echo "- docker-compose ps"
    echo "- docker logs synapse_gateway"
    echo "- docker exec synapse_postgres psql -U synapse -c '\d nodes'"
else
    echo "Annul√©."
fi
