#!/usr/bin/env python3
"""SynapseGrid Gateway - Version simplifi√©e sans aioredis"""

import asyncio
import json
import logging
import time
import hashlib
import uuid
import os
from typing import Dict, Any, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# Imports standards seulement
import redis
import psycopg2
from psycopg2.extras import RealDictCursor
from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI
app = FastAPI(title="SynapseGrid Gateway", version="3.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Models
class SubmitJobRequest(BaseModel):
    model_name: str
    input_data: Dict[str, Any]
    priority: int = 1

# Configuration
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", "6379"))
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres")
POSTGRES_DB = os.getenv("POSTGRES_DB", "synapse")
POSTGRES_USER = os.getenv("POSTGRES_USER", "synapse")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "synapse123")

# Connexions globales
redis_client = None
pg_conn = None
executor = ThreadPoolExecutor(max_workers=10)

def get_redis():
    """Get Redis connection"""
    return redis.StrictRedis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        decode_responses=True
    )

def get_postgres():
    """Get PostgreSQL connection"""
    return psycopg2.connect(
        host=POSTGRES_HOST,
        database=POSTGRES_DB,
        user=POSTGRES_USER,
        password=POSTGRES_PASSWORD,
        cursor_factory=RealDictCursor
    )

def simple_hash(text: str) -> str:
    """Simple hash function sans pgcrypto"""
    return hashlib.sha256(text.encode()).hexdigest()[:64]

def generate_job_id() -> str:
    """Generate job ID"""
    timestamp = int(time.time() * 1000)
    suffix = uuid.uuid4().hex[:8]
    return f"job_{timestamp}_{suffix}"

async def redis_async(func, *args):
    """Wrapper async pour Redis sync"""
    return await asyncio.get_event_loop().run_in_executor(
        executor, func, *args
    )

def check_table_columns(table_name: str) -> list:
    """V√©rifie les colonnes existantes d'une table"""
    try:
        with pg_conn.cursor() as cur:
            cur.execute("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = %s
            """, (table_name,))
            return [row['column_name'] for row in cur.fetchall()]
    except:
        return []

@app.on_event("startup")
async def startup():
    """Startup"""
    global redis_client, pg_conn
    
    logger.info("üöÄ D√©marrage du Gateway...")
    
    # Redis
    try:
        redis_client = get_redis()
        redis_client.ping()
        logger.info("‚úÖ Redis connect√©")
    except Exception as e:
        logger.error(f"‚ùå Erreur Redis: {e}")
    
    # PostgreSQL
    try:
        pg_conn = get_postgres()
        logger.info("‚úÖ PostgreSQL connect√©")
        
        # Afficher les colonnes disponibles
        for table in ['clients', 'jobs', 'nodes']:
            cols = check_table_columns(table)
            logger.info(f"Table {table}: {cols}")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur PostgreSQL: {e}")
    
    logger.info("üéâ Gateway d√©marr√©!")

@app.on_event("shutdown")
async def shutdown():
    """Shutdown"""
    if redis_client:
        redis_client.close()
    if pg_conn:
        pg_conn.close()
    if executor:
        executor.shutdown()

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "service": "SynapseGrid Gateway",
        "version": "3.0.0",
        "status": "online"
    }

@app.get("/health")
async def health():
    """Health check"""
    health_status = {"status": "healthy", "services": {}}
    
    # Test Redis
    try:
        await redis_async(redis_client.ping)
        health_status["services"]["redis"] = "healthy"
    except:
        health_status["services"]["redis"] = "unhealthy"
        health_status["status"] = "degraded"
    
    # Test PostgreSQL
    try:
        with pg_conn.cursor() as cur:
            cur.execute("SELECT 1")
        health_status["services"]["postgres"] = "healthy"
    except:
        health_status["services"]["postgres"] = "unhealthy"
        health_status["status"] = "degraded"
    
    return health_status

@app.post("/submit")
async def submit_job(
    request: SubmitJobRequest,
    authorization: str = Header(None),
    x_client_id: str = Header(None, alias="X-Client-ID")
):
    """Submit job avec gestion dynamique des colonnes"""
    
    # Validation basique
    if not authorization or not x_client_id:
        raise HTTPException(status_code=401, detail="Auth required")
    
    # Generate job ID
    job_id = generate_job_id()
    
    logger.info(f"üì• Job {job_id} de {x_client_id}")
    
    # Pr√©parer les donn√©es
    job_data = {
        "job_id": job_id,
        "client_id": x_client_id,
        "model_name": request.model_name,
        "input_data": json.dumps(request.input_data),
        "status": "queued",
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Sauvegarder dans PostgreSQL avec gestion dynamique
    try:
        with pg_conn.cursor() as cur:
            # V√©rifier les colonnes disponibles
            cols = check_table_columns('jobs')
            
            # Construire la requ√™te avec seulement les colonnes qui existent
            insert_cols = []
            insert_vals = []
            
            if 'job_id' in cols:
                insert_cols.append('job_id')
                insert_vals.append(job_id)
            
            if 'client_id' in cols:
                insert_cols.append('client_id')
                insert_vals.append(x_client_id)
                
            if 'model_name' in cols:
                insert_cols.append('model_name')
                insert_vals.append(request.model_name)
                
            if 'input_data' in cols:
                insert_cols.append('input_data')
                insert_vals.append(json.dumps(request.input_data))
                
            if 'status' in cols:
                insert_cols.append('status')
                insert_vals.append('queued')
            
            # Executer seulement si on a des colonnes
            if insert_cols:
                placeholders = ','.join(['%s'] * len(insert_cols))
                query = f"INSERT INTO jobs ({','.join(insert_cols)}) VALUES ({placeholders})"
                cur.execute(query, insert_vals)
                pg_conn.commit()
                logger.info(f"‚úÖ Job sauv√© en DB avec {len(insert_cols)} colonnes")
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur DB (non critique): {e}")
        # On continue m√™me si la DB √©choue
    
    # Sauvegarder dans Redis
    try:
        # Queue
        queue_key = f"jobs:queue:{x_client_id}"
        await redis_async(redis_client.lpush, queue_key, json.dumps(job_data))
        
        # Info
        info_key = f"job:{job_id}:info"
        await redis_async(redis_client.hmset, info_key, job_data)
        await redis_async(redis_client.expire, info_key, 3600)
        
        logger.info(f"‚úÖ Job {job_id} dans Redis")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur Redis: {e}")
        raise HTTPException(status_code=500, detail="Storage error")
    
    return {
        "job_id": job_id,
        "status": "queued",
        "message": "Job submitted successfully"
    }

@app.get("/job/{job_id}/status")
async def get_job_status(job_id: str):
    """Get job status avec fallback Redis/PostgreSQL"""
    
    # Essayer Redis d'abord
    try:
        info_key = f"job:{job_id}:info"
        job_data = await redis_async(redis_client.hgetall, info_key)
        
        if job_data:
            return {
                "job_id": job_id,
                "status": job_data.get("status", "unknown"),
                "created_at": job_data.get("created_at")
            }
    except:
        pass
    
    # Fallback PostgreSQL
    try:
        with pg_conn.cursor() as cur:
            # Requ√™te simple avec colonnes de base
            cur.execute("""
                SELECT job_id, status, created_at 
                FROM jobs 
                WHERE job_id = %s
            """, (job_id,))
            
            job = cur.fetchone()
            if job:
                return {
                    "job_id": job['job_id'],
                    "status": job.get('status', 'unknown'),
                    "created_at": str(job.get('created_at', ''))
                }
    except:
        pass
    
    raise HTTPException(status_code=404, detail="Job not found")


@app.post("/nodes/register")
async def register_node(node_data: dict):
    """Enregistrer un nouveau node"""
    node_id = node_data.get("node_id")
    node_type = node_data.get("node_type", "docker")
    region = node_data.get("region", "eu-west-1")
    
    if not node_id:
        raise HTTPException(status_code=400, detail="node_id required")
    
    try:
        # Enregistrer dans Redis
        node_key = f"node:{node_id}:{region}:info"
        node_info = {
            "node_id": node_id,
            "node_type": node_type,
            "region": region,
            "status": "online",
            "last_seen": datetime.utcnow().isoformat(),
            "capabilities": json.dumps(node_data.get("capabilities", {})),
            "max_concurrent": str(node_data.get("max_concurrent", 1))
        }
        
        # Utiliser le wrapper async pour Redis
        for key, value in node_info.items():
            await redis_async(redis_client.hset, node_key, key, value)
        await redis_async(redis_client.expire, node_key, 60)
        
        # Enregistrer dans PostgreSQL
        try:
            with pg_conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO nodes (node_id, node_type, region, status)
                    VALUES (%s, %s, %s, 'online')
                    ON CONFLICT (node_id) DO UPDATE
                    SET status = 'online', last_seen = CURRENT_TIMESTAMP
                """, (node_id, node_type, region))
                pg_conn.commit()
        except Exception as e:
            logger.warning(f"Erreur DB lors de l'enregistrement du node: {e}")
        
        logger.info(f"‚úÖ Node {node_id} enregistr√© ({node_type} dans {region})")
        return {"status": "registered", "node_id": node_id}
        
    except Exception as e:
        logger.error(f"‚ùå Erreur enregistrement node: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def get_metrics():
    """Endpoint de m√©triques pour Prometheus"""
    try:
        # R√©cup√©rer les m√©triques depuis Redis et PostgreSQL
        metrics = []
        
        # M√©trique: jobs en attente
        queue_length = await redis_async(redis_client.llen, "jobs:queue:eu-west-1")
        metrics.append(f"synapsegrid_jobs_queued{{region=\"eu-west-1\"}} {queue_length}")
        
        # M√©trique: nodes actifs
        try:
            with pg_conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM nodes WHERE status = 'online'")
                active_nodes = cur.fetchone()['count']
                metrics.append(f"synapsegrid_nodes_active {active_nodes}")
                
                cur.execute("SELECT COUNT(*) FROM jobs WHERE status = 'completed' AND completed_at > NOW() - INTERVAL '1 minute'")
                jobs_per_minute = cur.fetchone()['count']
                metrics.append(f"synapsegrid_jobs_completed_per_minute {jobs_per_minute}")
        except:
            pass
        
        # M√©trique: sant√© du service
        metrics.append("synapsegrid_gateway_up 1")
        
        return "\n".join(metrics)
        
    except Exception as e:
        logger.error(f"Erreur metrics: {e}")
        return "synapsegrid_gateway_up 0"

@app.get("/nodes")
async def list_nodes():
    """Lister tous les nodes actifs"""
    try:
        nodes = []
        
        # R√©cup√©rer depuis Redis
        pattern = "node:*:*:info"
        keys = []
        cursor = 0
        while True:
            cursor, batch_keys = await redis_async(redis_client.scan, cursor, match=pattern, count=100)
            keys.extend(batch_keys)
            if cursor == 0:
                break
        
        for key in keys:
            node_info = await redis_async(redis_client.hgetall, key)
            if node_info:
                nodes.append({
                    "node_id": node_info.get("node_id"),
                    "node_type": node_info.get("node_type"),
                    "region": node_info.get("region"),
                    "status": node_info.get("status"),
                    "last_seen": node_info.get("last_seen")
                })
        
        return {"nodes": nodes, "count": len(nodes)}
        
    except Exception as e:
        logger.error(f"Erreur list nodes: {e}")
        return {"nodes": [], "count": 0}

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8080,
        reload=True,
        log_level="info"
    )
