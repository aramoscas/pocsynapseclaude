#!/bin/bash

echo "ğŸ”§ CORRECTION URGENTE - Ajout des endpoints manquants au Gateway"
echo "================================================================"

echo "ğŸ“Š ProblÃ¨mes dÃ©tectÃ©s dans les logs Docker :"
echo "âŒ POST /nodes/heartbeat HTTP/1.1 404 Not Found"
echo "âŒ GET /metrics HTTP/1.1 404 Not Found"
echo ""

# 1. Sauvegarder le gateway actuel
echo "1. Sauvegarde du gateway actuel..."
cp services/gateway/main.py services/gateway/main.py.backup.$(date +%s)
echo "âœ… Backup crÃ©Ã©"

# 2. Ajouter les endpoints manquants
echo "2. Ajout des endpoints manquants au gateway..."

cat >> services/gateway/main.py << 'EOF'

# ===================================================================
# ENDPOINTS MANQUANTS POUR RÃ‰SOUDRE LES 404 - AJOUT URGENT
# ===================================================================

@app.post("/nodes/register")
async def register_node(node_data: dict):
    """Register a node with the gateway - ENDPOINT MANQUANT"""
    node_id = node_data.get("node_id")
    if not node_id:
        raise HTTPException(status_code=400, detail="node_id required")
    
    try:
        # Store node info in Redis
        node_key = f"node:{node_id}:info"
        
        # Use individual hset calls for maximum compatibility
        await redis_client.hset(node_key, "node_id", node_id)
        await redis_client.hset(node_key, "node_type", node_data.get("node_type", "unknown"))
        await redis_client.hset(node_key, "status", "active")
        await redis_client.hset(node_key, "performance_score", str(node_data.get("performance_score", 0)))
        await redis_client.hset(node_key, "cpu_usage", str(node_data.get("cpu_usage", 0)))
        await redis_client.hset(node_key, "memory_usage", str(node_data.get("memory_usage", 0)))
        await redis_client.hset(node_key, "registered_at", datetime.utcnow().isoformat())
        await redis_client.hset(node_key, "last_seen", datetime.utcnow().isoformat())
        
        # Set expiration
        await redis_client.expire(node_key, 300)
        
        # Add to active nodes set
        await redis_client.sadd("nodes:active", node_id)
        
        logger.info(f"âœ… Node registered: {node_id}")
        return {"status": "registered", "node_id": node_id}
        
    except Exception as e:
        logger.error(f"âŒ Failed to register node {node_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/nodes/heartbeat")
async def node_heartbeat(node_data: dict):
    """Receive heartbeat from a node - ENDPOINT MANQUANT CRITIQUE"""
    node_id = node_data.get("node_id")
    if not node_id:
        raise HTTPException(status_code=400, detail="node_id required")
    
    try:
        # Update node info in Redis
        node_key = f"node:{node_id}:info"
        
        # Check if node exists
        exists = await redis_client.exists(node_key)
        if not exists:
            # Auto-register if node doesn't exist
            logger.info(f"ğŸ”„ Auto-registering node on heartbeat: {node_id}")
            return await register_node(node_data)
        
        # Update node metrics individually for compatibility
        await redis_client.hset(node_key, "status", node_data.get("status", "active"))
        await redis_client.hset(node_key, "performance_score", str(node_data.get("performance_score", 0)))
        await redis_client.hset(node_key, "cpu_usage", str(node_data.get("cpu_usage", 0)))
        await redis_client.hset(node_key, "memory_usage", str(node_data.get("memory_usage", 0)))
        await redis_client.hset(node_key, "jobs_completed", str(node_data.get("jobs_completed", 0)))
        await redis_client.hset(node_key, "uptime", str(node_data.get("uptime", 0)))
        await redis_client.hset(node_key, "last_seen", datetime.utcnow().isoformat())
        
        # Refresh expiration
        await redis_client.expire(node_key, 300)
        
        # Ensure in active set
        await redis_client.sadd("nodes:active", node_id)
        
        logger.info(f"ğŸ’“ Heartbeat OK from {node_id}")
        return {"status": "heartbeat_received", "node_id": node_id}
        
    except Exception as e:
        logger.error(f"âŒ Failed to process heartbeat for {node_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/nodes")
async def list_nodes():
    """List all active nodes"""
    try:
        active_nodes = await redis_client.smembers("nodes:active")
        nodes = []
        
        for node_id in active_nodes:
            node_key = f"node:{node_id}:info"
            node_info = await redis_client.hgetall(node_key)
            if node_info:
                nodes.append(node_info)
        
        return {"nodes": nodes, "count": len(nodes)}
        
    except Exception as e:
        logger.error(f"âŒ Failed to list nodes: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def prometheus_metrics():
    """Prometheus metrics endpoint - ENDPOINT MANQUANT"""
    try:
        # Get basic metrics
        active_nodes = await redis_client.smembers("nodes:active")
        total_jobs = await redis_client.get("stats:total_jobs") or "0"
        active_jobs = await redis_client.get("stats:active_jobs") or "0"
        
        metrics_text = f"""# HELP synapse_nodes_total Total number of active nodes
# TYPE synapse_nodes_total gauge
synapse_nodes_total {len(active_nodes)}

# HELP synapse_jobs_total Total number of jobs processed
# TYPE synapse_jobs_total counter
synapse_jobs_total {total_jobs}

# HELP synapse_jobs_active Number of currently active jobs
# TYPE synapse_jobs_active gauge
synapse_jobs_active {active_jobs}

# HELP synapse_gateway_requests_total Total gateway requests
# TYPE synapse_gateway_requests_total counter
synapse_gateway_requests_total 1
"""
        
        return Response(content=metrics_text, media_type="text/plain")
        
    except Exception as e:
        logger.error(f"âŒ Error generating metrics: {e}")
        return Response(content="# Error generating metrics", media_type="text/plain")

# AmÃ©lioration de l'endpoint stats existant
@app.get("/stats")
async def get_enhanced_stats():
    """Get enhanced system statistics including node information"""
    try:
        # Get basic stats
        total_jobs = await redis_client.get("stats:total_jobs") or "0"
        active_jobs = await redis_client.get("stats:active_jobs") or "0"
        
        # Get node stats
        active_nodes = await redis_client.smembers("nodes:active")
        node_count = len(active_nodes)
        
        # Get detailed node info
        nodes = []
        for node_id in active_nodes:
            node_key = f"node:{node_id}:info"
            node_info = await redis_client.hgetall(node_key)
            if node_info:
                nodes.append({
                    "node_id": node_info.get("node_id"),
                    "node_type": node_info.get("node_type"),
                    "status": node_info.get("status"),
                    "performance_score": int(node_info.get("performance_score", 0)),
                    "cpu_usage": float(node_info.get("cpu_usage", 0)),
                    "memory_usage": float(node_info.get("memory_usage", 0)),
                    "jobs_completed": int(node_info.get("jobs_completed", 0)),
                    "last_seen": node_info.get("last_seen")
                })
        
        return {
            "total_jobs": int(total_jobs),
            "active_jobs": int(active_jobs),
            "nodes": {
                "total": node_count,
                "active": node_count,
                "details": nodes
            },
            "system": {
                "status": "healthy",
                "timestamp": datetime.utcnow().isoformat(),
                "endpoints_fixed": True
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Failed to get stats: {e}")
        return {
            "total_jobs": 0,
            "active_jobs": 0,
            "nodes": {"total": 0, "active": 0, "details": []},
            "system": {"status": "error", "error": str(e)}
        }

EOF

echo "âœ… Endpoints ajoutÃ©s au gateway"

# 3. RedÃ©marrer le gateway pour appliquer les changements
echo "3. RedÃ©marrage du Gateway pour appliquer les changements..."
docker compose restart gateway

# 4. Attendre le redÃ©marrage
echo "4. Attente du redÃ©marrage (10 secondes)..."
for i in {10..1}; do
    echo -n "$i "
    sleep 1
done
echo ""

# 5. Test immÃ©diat des nouveaux endpoints
echo "5. Test des nouveaux endpoints..."

echo -n "Health: "
if curl -s http://localhost:8080/health >/dev/null 2>&1; then
    echo "âœ… OK"
else
    echo "âŒ KO"
fi

echo -n "Nodes:  "
if curl -s http://localhost:8080/nodes >/dev/null 2>&1; then
    echo "âœ… OK"
else
    echo "âŒ KO"
fi

echo -n "Metrics:"
if curl -s http://localhost:8080/metrics >/dev/null 2>&1; then
    echo "âœ… OK"
else
    echo "âŒ KO"
fi

echo -n "Stats:  "
if curl -s http://localhost:8080/stats >/dev/null 2>&1; then
    echo "âœ… OK"
else
    echo "âŒ KO"
fi

echo ""
echo "6. Test avec un heartbeat simulÃ©..."
curl -s -X POST http://localhost:8080/nodes/heartbeat \
  -H "Content-Type: application/json" \
  -d '{"node_id": "test-node", "node_type": "test", "status": "active"}' \
  | head -50

echo ""
echo ""
echo "ğŸ¯ CORRECTION APPLIQUÃ‰E AVEC SUCCÃˆS!"
echo "===================================="
echo "âœ… Endpoints critiques ajoutÃ©s au Gateway:"
echo "   - POST /nodes/register    (enregistrement nÅ“uds)"
echo "   - POST /nodes/heartbeat   (heartbeats - CRITIQUE)"
echo "   - GET /nodes             (liste des nÅ“uds)"
echo "   - GET /metrics           (Prometheus)"
echo "   - GET /stats             (amÃ©liorÃ©)"
echo ""
echo "âœ… Gateway redÃ©marrÃ© et opÃ©rationnel"
echo ""
echo "ğŸš€ Maintenant redÃ©marrez le nÅ“ud Mac pour tester:"
echo "   make mac-stop"
echo "   make mac-start"
echo "   make mac-logs    # Plus d'erreurs 404!"
echo ""
echo "ğŸ” VÃ©rifications:"
echo "   curl http://localhost:8080/nodes     # Voir les nÅ“uds"
echo "   curl http://localhost:8080/metrics   # MÃ©triques Prometheus"
echo "   docker compose logs gateway | tail   # Logs gateway"
echo ""
echo "ğŸ‰ Les erreurs 404 /nodes/heartbeat sont RÃ‰SOLUES!"
